{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d848f7-9b46-4123-bb07-184ecc0818d5",
   "metadata": {},
   "source": [
    "# DATA SCIENCE CHALLENGE #3 - SIMILITUD ENTRE PRODUCTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca007ae6-54df-48f8-8e88-e504c57216fa",
   "metadata": {},
   "source": [
    "+ Un desafío constante en MELI es el de poder agrupar productos similares utilizando algunos atributos de estos como pueden ser el título, la descripción o su imagen;\n",
    "+ Para este desafío tenemos un dataset `items_titles.csv` que tiene títulos de 30 mil productos de 3 categorías diferentes de Mercado Libre Brasil;\n",
    "+ El objetivo del desafío es poder generar una Jupyter notebook que determine cuán similares son dos títulos del dataset `item_titles_test.csv`, generando como output un listado de la forma:\n",
    "\n",
    "|    ITE_ITEM_TITLE   |   ITE_ITEM_TITLE    |  Score Similitud (0,1)  |\n",
    "|:-------------------:|:-------------------:|:-----------------------:|\n",
    "|   Zapatillas Nike   |  Zapatillas Adidas  |           0.5           |\n",
    "|   Zapatillas Nike   |  Zapatillas Nike    |            1            |\n",
    "\n",
    "donde ordenando por score de similitud podamos encontrar los pares de productos más similares en nuestro dataset de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc5e11-332b-4824-8e66-04e20792f0c7",
   "metadata": {},
   "source": [
    "# Loading modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dca07cb-2e44-48b8-aa24-f61afe12c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframes\n",
    "import numpy as np\n",
    "from unidecode import unidecode # special characters\n",
    "from collections import defaultdict # dictionary\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722cb29-608c-4ac0-a144-1e4799a8f10e",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd021d6-4218-45ce-a229-d8b6422b91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset containing the instances that will be used to develop the algorithm\n",
    "df_train = pd.read_csv(filepath_or_buffer = '../data/items_titles.csv')\n",
    "# loading the dataset containing the instances that will be used to test the algorithm\n",
    "df_test = pd.read_csv(filepath_or_buffer = '../data/items_titles_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e735b-caae-46ac-af8b-7ec413a44a0b",
   "metadata": {},
   "source": [
    "## Pre-processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2fea-f954-4bb5-aa14-b8cc87a1fbad",
   "metadata": {},
   "source": [
    "Creating a function to pre-process each sentence into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69c5528-d055-4f5a-a725-c3e91a52135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(sentence):\n",
    "    \"\"\"\n",
    "    Pre-processes the sentence by lower casing all characters, replacing any special characters by their standard \n",
    "    counterparts (e.g., 'ê' -> e) and splitting the sentence based on whitespace\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sentence: string\n",
    "        A string containing the sentence that will be pre-processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        A list of strings where each element is a word found in the original sentence\n",
    "    \"\"\"\n",
    "    decoded_sentence = unidecode(sentence) # stripping special characters\n",
    "    decoded_sentence = decoded_sentence.lower() # lower casing the entire sentence\n",
    "    decoded_sentence = re.sub('[^\\w\\s\\d]', ' ', decoded_sentence) # removing any punctuation\n",
    "    return word_tokenize(text = decoded_sentence, language = \"portuguese\") # returning the tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ce3370-2ea6-4724-98ec-5751304d4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of words within each sentence\n",
    "train_sentences = df_train.ITE_ITEM_TITLE.apply(pre_process_text).tolist()\n",
    "## getting the list of unique tokens across all sentences\n",
    "vocabulary = set([word for sentence in train_sentences for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da0bf84-4af7-4526-8bec-b9d2472e6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vocabulary(vocabulary, unknown_idx = 0, digit_idx = 1):\n",
    "    \"\"\"\n",
    "    Integer-encode a vocabulary.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocabulary: dict\n",
    "        Input dictionary containing the tokens found in a corpus.\n",
    "    digit_idx: int, defaults to 1\n",
    "        Integer used to encode any token that contains a digit.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    word_to_idx: dict\n",
    "        Dictionary used to encode each token to a unique integer\n",
    "    idx_to_word: dict\n",
    "        Dictionary used to encode each integer to a token\n",
    "    \"\"\"\n",
    "    # creating default dictionaries to store the encoded values\n",
    "    word_to_idx = defaultdict(int)\n",
    "    idx_to_word = defaultdict(int)\n",
    "    \n",
    "    # initializing the idx_to_word dictionary with the values for the unknown and digit ids\n",
    "    idx_to_word[unknown_idx] = '__UNK__'\n",
    "    idx_to_word[digit_idx] = '__DGT__'\n",
    "    \n",
    "    #\n",
    "    counter = unknown_idx + digit_idx + 1\n",
    "    \n",
    "    # iterating across the non-digit tokens in the vocabulary to assign an index to them\n",
    "    for token in vocabulary:\n",
    "        if token.isdigit() or any(character.isdigit() for character in token):\n",
    "            pass\n",
    "        else:\n",
    "            word_to_idx[token] = counter\n",
    "            idx_to_word[counter] = token\n",
    "            counter += 1\n",
    "    return word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e53fe-3769-4351-b580-d4fc24f608e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the default indexes that will be used for the unknown and digit tokens\n",
    "idx_for_unknown = 0\n",
    "idx_for_digits = 1\n",
    "# getting the encoded representations of tokens and their indexes\n",
    "word2idx, idx2word = encode_vocabulary(vocabulary = vocabulary, unknown_idx = idx_for_unknown, digit_idx = idx_for_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0838f5f5-84bb-4a24-bf12-19e66f0bfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, word_to_idx_dict, unknown_idx = 0, digit_idx = 1):\n",
    "    \"\"\"\n",
    "    Integer-encode an input sentence based on the indexes given to each token in a vocabulary, while also taking\n",
    "    care to given specific encoding values to unknown words and for any token where a digit is found.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    # pre-processing and tokenizing the sentence\n",
    "    tokenized_sentence = pre_process_text(sentence)\n",
    "    # creating and empty list to store the indexes of the sentence tokens\n",
    "    sentence_idx = []\n",
    "    # iterating across the tokens in the tokenized sentence to extract their indexes\n",
    "    for token in tokenized_sentence:\n",
    "        # assigning any token containing a digit to its own integer index\n",
    "        if token.isdigit() or any(character.isdigit() for character in token):\n",
    "            sentence_idx.append(idx_for_digits)\n",
    "        # using the get method of the dictionary to extract the word index or, if that does not exist, assigning\n",
    "        # the unknown word index\n",
    "        else:\n",
    "            sentence_idx.append(word2idx.get(token, idx_for_unknown))\n",
    "    \n",
    "    return sentence_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8b897-1456-48e0-9ef7-40a86aa42f31",
   "metadata": {},
   "source": [
    "## Defining a baseline for the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c1cac07d-4114-44f5-ad9b-950f58852b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_array(indexes, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a one-hot vector representation of an input sentence that have already been encoded.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    indexes: list of integers\n",
    "        List of integers representing the indexes from each of the tokens found in a given sentence\n",
    "    vocab_size: int\n",
    "        Size of the original vocabulary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        A numpy array of shape (vocab_size,) containing the vector representation for a sentence represented by\n",
    "        a sequence of tokens\n",
    "    \"\"\"\n",
    "    # creating a matrix of zeroes to accumulate the one hot encoded representation of each token in the sentence\n",
    "    # independently - each row is a token, each column a token index\n",
    "    one_hot = np.zeros((len(indexes), vocab_size))\n",
    "    # populating the zeroes matrix according to the number of times each token appears in the input sentence\n",
    "    for row, idx in enumerate(indexes):\n",
    "        one_hot[row][idx] += 1\n",
    "    # creating a vector representation of the input sentence based on the column-wise average of the one hot encoding\n",
    "    # this array represents the average content of the sentence\n",
    "    return np.mean(one_hot, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f598eaad-2874-47e6-bc1f-0e0ce9bd26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_similitud(target_sentence, possible_sentences, sentence_encoder, word_to_idx_dict, unknown_idx, digit_idx, ohe_function, n_closest = 5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # extracting the vocabulary size from the word to index dictionary - this is the maximum index value in this dictionary\n",
    "    vocab_size = max(word_to_idx_dict.values())\n",
    "    # encoding the target sentence\n",
    "    target_tokens = sentence_encoder(target_sentence, word_to_idx_dict, unknown_idx, digit_idx)\n",
    "    # one hot encoding the target sentence\n",
    "    ohe_target = ohe_function(indexes = target_tokens, vocab_size = vocab_size)\n",
    "    \n",
    "    # creating an empty list to store the tuple containing the cosine similarity between the target setence and\n",
    "    # every other sentence in possible_sentences\n",
    "    sentence_similarity = []\n",
    "    \n",
    "    # iterating across each of the other possible sentences\n",
    "    for proposal in possible_sentences:\n",
    "        # encoding the proposed sentence\n",
    "        proposal_tokens = sentence_encoder(proposal, word_to_idx_dict, unknown_idx, digit_idx)\n",
    "        # one hot encoding the proposed sentence\n",
    "        ohe_proposal = ohe_function(indexes = proposal_tokens, vocab_size = vocab_size)\n",
    "        \n",
    "        # calculating the cosine similarity between the vector representation of the target sentence and the \n",
    "        # proposed sentence\n",
    "        numerator = np.dot(ohe_target[1:], ohe_proposal[1:])\n",
    "        denominator = np.linalg.norm(ohe_target[1:]) * np.linalg.norm(ohe_proposal[1:])\n",
    "        cosine = numerator / denominator if not np.isclose(0.0, denominator) else 0.0\n",
    "        \n",
    "        # appending the tuple containing the proposed sentence and the cosine similarity between that sentence\n",
    "        # and the target sentence into the list of sentence similarities\n",
    "        sentence_similarity.append((proposal, cosine))\n",
    "    \n",
    "    # reordering the list of sentence similarity so that tuples with higher cosine similarity comes first, and\n",
    "    # extracting the n_closest matches to the target sentence according to this metric\n",
    "    sentence_similarity = sorted(sentence_similarity, key = lambda x: x[-1], reverse = True)[:n_closest]\n",
    "    \n",
    "    return sentence_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c42b5cd7-3846-417a-8a6c-3421dd159481",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_scoring = score_similitud(\n",
    "    df_test.iloc[0, 0], possible_sentences = df_test.iloc[1:, 0], \n",
    "    sentence_encoder = encode_sentence, word_to_idx_dict = word2idx, \n",
    "    unknown_idx = idx_for_unknown, digit_idx = idx_for_digits, \n",
    "    ohe_function = one_hot_array, n_closest = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6dba72d7-1b42-4124-a26b-15b0acb87de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = pd.DataFrame(sample_scoring, columns = ['Similar Items', 'Score Similitud'])\n",
    "sample_output['Target Item'] = df_test.iloc[0, 0]\n",
    "sample_output = sample_output[['Target Item', 'Similar Items', 'Score Similitud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5c361-7f55-4181-b41a-51dbe39341eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
