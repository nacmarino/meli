{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d848f7-9b46-4123-bb07-184ecc0818d5",
   "metadata": {},
   "source": [
    "# DATA SCIENCE CHALLENGE #3 - SIMILITUD ENTRE PRODUCTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca007ae6-54df-48f8-8e88-e504c57216fa",
   "metadata": {},
   "source": [
    "+ Un desafío constante en MELI es el de poder agrupar productos similares utilizando algunos atributos de estos como pueden ser el título, la descripción o su imagen;\n",
    "+ Para este desafío tenemos un dataset `items_titles.csv` que tiene títulos de 30 mil productos de 3 categorías diferentes de Mercado Libre Brasil;\n",
    "+ El objetivo del desafío es poder generar una Jupyter notebook que determine cuán similares son dos títulos del dataset `item_titles_test.csv`, generando como output un listado de la forma:\n",
    "\n",
    "|    ITE_ITEM_TITLE   |   ITE_ITEM_TITLE    |  Score Similitud (0,1)  |\n",
    "|:-------------------:|:-------------------:|:-----------------------:|\n",
    "|   Zapatillas Nike   |  Zapatillas Adidas  |           0.5           |\n",
    "|   Zapatillas Nike   |  Zapatillas Nike    |            1            |\n",
    "\n",
    "donde ordenando por score de similitud podamos encontrar los pares de productos más similares en nuestro dataset de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad2623-8be0-464d-91d5-a42a740440fc",
   "metadata": {},
   "source": [
    "## OVERVIEW OF THE FIRST PROPOSED SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f4145-2886-4f89-85b6-e1e6d87f220d",
   "metadata": {},
   "source": [
    "This dataset provides a subset of the items likely to be found in production, and none of the datasets provide annotations of which products are known to be similar to other products. Yet, the task is that given any two items with their descriptions, we need to output a score between 0 and 1 indicating how much similar are those two items given their descriptions. From the example above, we can see that if half of the words are shared between items, then their similar should be 0.5. \n",
    "\n",
    "There are quite a few ways to tackle this task, but I believe the first attempt should be the simplest one possible - so that we can know whether any more ellaborate approach is bringing further value or not. As such, I decided to implement a brute force algorithm that will create simple vector representations for each item based on its description and, based on the cosine similarity between those vectors, will find the _n_ items whose descriptions are the most similar ones. The end product of this algoritm will be a dataframe containing each pair of items and their similarity score in each row.\n",
    "\n",
    "An important aspect of the proposed approach is that we will use the training data to build the vocabulary used by the algorithm - taking care of unknown tokens and also assigning a special token to any word containing a digit. Finally, if put to production, this algorithm would work by receiving an input description and looking at its knowledge base for the other items whose descriptions are close to it - as such, it would not work by crossing all items with all other items in its knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc5e11-332b-4824-8e66-04e20792f0c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dca07cb-2e44-48b8-aa24-f61afe12c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframes\n",
    "import numpy as np # arrays\n",
    "import matplotlib.pyplot as plt # plotting utilities\n",
    "from unidecode import unidecode # special characters\n",
    "from collections import defaultdict # dictionary\n",
    "from nltk.tokenize import word_tokenize # sentence tokenization\n",
    "from collections import Counter # counter dictionary \n",
    "import re # regex\n",
    "import nltk # text\n",
    "import os # operational system\n",
    "from tqdm import tqdm # progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722cb29-608c-4ac0-a144-1e4799a8f10e",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd021d6-4218-45ce-a229-d8b6422b91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset containing the instances that will be used to develop the algorithm\n",
    "df_train = pd.read_csv(filepath_or_buffer = '../data/items_titles.csv')\n",
    "# loading the dataset containing the instances that will be used to test the algorithm\n",
    "df_test = pd.read_csv(filepath_or_buffer = '../data/items_titles_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e735b-caae-46ac-af8b-7ec413a44a0b",
   "metadata": {},
   "source": [
    "## Pre-processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2fea-f954-4bb5-aa14-b8cc87a1fbad",
   "metadata": {},
   "source": [
    "Creating a function to pre-process and tokenize each sentence into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69c5528-d055-4f5a-a725-c3e91a52135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(sentence):\n",
    "    \"\"\"\n",
    "    Pre-processes the sentence by lower casing all characters, replacing any special characters by their standard \n",
    "    counterparts (e.g., 'ê' -> e) and splitting the sentence based on whitespace.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sentence: string\n",
    "        A string containing the sentence that will be pre-processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        A list of strings where each element is a word found in the original sentence\n",
    "    \"\"\"\n",
    "    decoded_sentence = unidecode(sentence) # stripping special characters\n",
    "    decoded_sentence = decoded_sentence.lower() # lower casing the entire sentence\n",
    "    decoded_sentence = re.sub('[^\\w\\s\\d]', ' ', decoded_sentence) # removing any punctuation\n",
    "    return word_tokenize(text = decoded_sentence, language = \"portuguese\") # returning the tokenized sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59bfd3-5c69-45f6-b1bf-f6c7d2309657",
   "metadata": {},
   "source": [
    "Next, we will tokenize all sentences with the item descriptions from the training dataset, and use those tokens to create the vocabulary that will be used to score the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ce3370-2ea6-4724-98ec-5751304d4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of words within each sentence\n",
    "train_sentences = df_train.ITE_ITEM_TITLE.apply(pre_process_text).tolist()\n",
    "## getting the list of unique tokens across all sentences\n",
    "vocabulary = set([word for sentence in train_sentences for word in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b477cf1-15d0-49d8-8d01-cea4c704f8a5",
   "metadata": {},
   "source": [
    "We will create a function to encode the training vobulary, which will give an integer index to each token, a special index to any token containing a digit and will also create a special token and index for unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da0bf84-4af7-4526-8bec-b9d2472e6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_vocabulary(vocabulary, unknown_idx = 0, digit_idx = 1):\n",
    "    \"\"\"\n",
    "    Integer-encode a vocabulary.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocabulary: dict\n",
    "        Input dictionary containing the tokens found in a corpus.\n",
    "    digit_idx: int, defaults to 1\n",
    "        Integer encoding the index that will be given to any token containing a digit\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word_to_idx: dict\n",
    "        Dictionary used to encode each token to a unique integer.\n",
    "    idx_to_word: dict\n",
    "        Dictionary used to encode each integer to a token.\n",
    "    \"\"\"\n",
    "    # creating default dictionaries to store the encoded values\n",
    "    word_to_idx = defaultdict(int)\n",
    "    idx_to_word = defaultdict(int)\n",
    "    \n",
    "    # initializing the idx_to_word dictionary with the values for the unknown and digit ids\n",
    "    idx_to_word[unknown_idx] = '__UNK__'\n",
    "    idx_to_word[digit_idx] = '__DGT__'\n",
    "    \n",
    "    #\n",
    "    counter = unknown_idx + digit_idx + 1\n",
    "    \n",
    "    # iterating across the non-digit tokens in the vocabulary to assign an index to them\n",
    "    for token in vocabulary:\n",
    "        if token.isdigit() or any(character.isdigit() for character in token):\n",
    "            pass\n",
    "        else:\n",
    "            word_to_idx[token] = counter\n",
    "            idx_to_word[counter] = token\n",
    "            counter += 1\n",
    "    return word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc1050-9474-43bf-95a1-65a2bf7ce5a0",
   "metadata": {},
   "source": [
    "Creating our training vocabulary, containing all words that appear in the train dataset, along with their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e53fe-3769-4351-b580-d4fc24f608e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the default indexes that will be used for the unknown and digit tokens\n",
    "idx_for_unknown = 0\n",
    "idx_for_digits = 1\n",
    "# getting the encoded representations of tokens and their indexes\n",
    "word2idx, idx2word = encode_vocabulary(vocabulary = vocabulary, unknown_idx = idx_for_unknown, digit_idx = idx_for_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8b897-1456-48e0-9ef7-40a86aa42f31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementing a baseline approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df7611-fa68-4032-91c7-0df977130852",
   "metadata": {},
   "source": [
    "Creating a series of functions that will:  \n",
    "\n",
    "+ Get an input sentence and encode it into the indexes of its tokens - this is based on the tokens seem in the training dataset;\n",
    "+ Get an input list of token indexes and create a simple vector representation from it;\n",
    "+ Find the most similar itens to a given target item according to their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0838f5f5-84bb-4a24-bf12-19e66f0bfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, word_to_idx_dict, unknown_idx = 0, digit_idx = 1):\n",
    "    \"\"\"\n",
    "    Integer-encode an input sentence based on the indexes given to each token in a vocabulary, while also taking\n",
    "    care to given specific encoding values to unknown words and for any token where a digit is found.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    sentence: string\n",
    "        A string containing the input sentence that will be encoded.\n",
    "    word_to_idx_dict: dict\n",
    "        Dictionary mapping the tokens to their integer indexes.\n",
    "    unknown_idx: int, defaults to 0\n",
    "        Integer encoding the index that will be given to any unknown token\n",
    "    digit_idx: int, defaults to 1\n",
    "        Integer encoding the index that will be given to any token containing a digit\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence_idx: list of int\n",
    "        List of integers representing the indexes from each of the tokens found in a given sentence\n",
    "    \"\"\"\n",
    "    # pre-processing and tokenizing the sentence\n",
    "    tokenized_sentence = pre_process_text(sentence)\n",
    "    # creating and empty list to store the indexes of the sentence tokens\n",
    "    sentence_idx = []\n",
    "    # iterating across the tokens in the tokenized sentence to extract their indexes\n",
    "    for token in tokenized_sentence:\n",
    "        # assigning any token containing a digit to its own integer index\n",
    "        if token.isdigit() or any(character.isdigit() for character in token):\n",
    "            sentence_idx.append(digit_idx)\n",
    "        # using the get method of the dictionary to extract the word index or, if that does not exist, assigning\n",
    "        # the unknown word index\n",
    "        else:\n",
    "            sentence_idx.append(word2idx.get(token, unknown_idx))\n",
    "    \n",
    "    return sentence_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cac07d-4114-44f5-ad9b-950f58852b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_array(indexes, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a one-hot vector representation of an input sentence that have already been encoded.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    indexes: list of int\n",
    "        List of integers representing the indexes from each of the tokens found in a given sentence.\n",
    "    vocab_size: int\n",
    "        Size of the original vocabulary.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        A numpy array of shape (vocab_size,) containing the vector representation for a sentence represented by\n",
    "        a sequence of tokens.\n",
    "    \"\"\"\n",
    "    # creating a matrix of zeroes to accumulate the one hot encoded representation of each token in the sentence\n",
    "    # independently - each row is a token, each column a token index\n",
    "    one_hot = np.zeros((len(indexes), vocab_size))\n",
    "    # populating the zeroes matrix according to the number of times each token appears in the input sentence\n",
    "    for row, idx in enumerate(indexes):\n",
    "        one_hot[row][idx] += 1\n",
    "    # creating a vector representation of the input sentence based on the column-wise average of the one hot encoding\n",
    "    # this array represents the average content of the sentence\n",
    "    return np.mean(one_hot, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f598eaad-2874-47e6-bc1f-0e0ce9bd26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_similitud(target_sentence, possible_sentences, sentence_encoder, word_to_idx_dict, unknown_idx, digit_idx, ohe_function, n_closest = 5):\n",
    "    \"\"\"\n",
    "    Extract the itens with the most similar descriptions given a target item, according to the cosine similarity between their\n",
    "    vector representations.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    target_sentence: string\n",
    "        A string containing the target sentence. It represents the item we are interested in finding other similar to it.\n",
    "    possible_sentences: list of strings\n",
    "        List containing the sentences that represent the itens that will be used to search for those that most closely \n",
    "        matches with the sentence in target_sentence.\n",
    "    sentence_encoder: python function\n",
    "        Python function used to integer-encode the target sentence.\n",
    "    word_to_idx_dict: dict\n",
    "        Dictionary mapping the tokens to their integer indexes.\n",
    "    unknown_idx: int, defaults to 0\n",
    "        Integer encoding the index that will be given to any unknown token.\n",
    "    digit_idx: int, defaults to 1\n",
    "        Integer encoding the index that will be given to any token containing a digit.\n",
    "    ohe_function: python function\n",
    "        Python function used to one hot encode a list of token indexes.\n",
    "    n_closest: int, defaults to 5\n",
    "        Integer specifying the number of similar itens that will be returned by the function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence_similarity: list of tuples\n",
    "        Each tuple contains the item name and its cosine similarity with the target item used as an argument in target_sentence.\n",
    "        This length of this list is equal to the number of itens similar to the target item, which can be tuned by the function\n",
    "        argument `n_closest`; in addition, the tuples are ordered from that with the largest cosine similarity to the one with\n",
    "        the lowest, such that the first item in the list is the one is expected to be the most similar with the target item.\n",
    "    \"\"\"\n",
    "    # extracting the vocabulary size from the word to index dictionary - this is the maximum index value in this dictionary\n",
    "    max_token_idx = max(word_to_idx_dict.values())\n",
    "    # encoding the target sentence\n",
    "    target_tokens = sentence_encoder(target_sentence, word_to_idx_dict, unknown_idx, digit_idx)\n",
    "    # one hot encoding the target sentence\n",
    "    ohe_target = ohe_function(indexes = target_tokens, vocab_size = max_token_idx + 1)\n",
    "    \n",
    "    # creating an empty list to store the tuple containing the cosine similarity between the target setence and\n",
    "    # every other sentence in possible_sentences\n",
    "    sentence_similarity = []\n",
    "    \n",
    "    # iterating across each of the other possible sentences\n",
    "    for proposal in possible_sentences:\n",
    "        # encoding the proposed sentence\n",
    "        proposal_tokens = sentence_encoder(proposal, word_to_idx_dict, unknown_idx, digit_idx)\n",
    "        # one hot encoding the proposed sentence\n",
    "        ohe_proposal = ohe_function(indexes = proposal_tokens, vocab_size = max_token_idx + 1)\n",
    "        \n",
    "        # calculating the cosine similarity between the vector representation of the target sentence and the \n",
    "        # proposed sentence\n",
    "        numerator = np.dot(ohe_target[1:], ohe_proposal[1:])\n",
    "        denominator = np.linalg.norm(ohe_target[1:]) * np.linalg.norm(ohe_proposal[1:])\n",
    "        cosine = numerator / denominator if not np.isclose(0.0, denominator) else 0.0\n",
    "        \n",
    "        # appending the tuple containing the proposed sentence and the cosine similarity between that sentence\n",
    "        # and the target sentence into the list of sentence similarities\n",
    "        sentence_similarity.append((proposal, cosine))\n",
    "    \n",
    "    # reordering the list of sentence similarity so that tuples with higher cosine similarity comes first, and\n",
    "    # extracting the n_closest matches to the target sentence according to this metric\n",
    "    sentence_similarity = sorted(sentence_similarity, key = lambda x: x[-1], reverse = True)[:n_closest]\n",
    "    \n",
    "    return sentence_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc7066-65ee-4846-8660-4b3068b513a6",
   "metadata": {},
   "source": [
    "Finding each of the itens that are most similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5c8891-cb5d-4f11-8ec8-9e585f0593da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:38<00:00,  7.73s/it]\n"
     ]
    }
   ],
   "source": [
    "## creating an empty dataframe to store the results of the brute force item similarity search\n",
    "sample_output = pd.DataFrame()\n",
    "\n",
    "## iterating across each of the itens available in the test dataset\n",
    "for item in tqdm(df_test.iloc[:, 0].tolist()[:5]):\n",
    "    # extracting the five most similar itens to each other item\n",
    "    item_scoring = score_similitud(\n",
    "        item, possible_sentences = df_train.iloc[:, 0].tolist(), \n",
    "        sentence_encoder = encode_sentence, word_to_idx_dict = word2idx, \n",
    "        unknown_idx = idx_for_unknown, digit_idx = idx_for_digits, \n",
    "        ohe_function = one_hot_array, n_closest = 5\n",
    "    )\n",
    "    # tidying up the results to keep track of the item pairs\n",
    "    item_output = pd.DataFrame(item_scoring, columns = ['Similar Items', 'Score Similitud'])\n",
    "    item_output['Target Item'] = item\n",
    "    item_output = item_output[['Target Item', 'Similar Items', 'Score Similitud']]\n",
    "    # concatenating the output of the current iteration with the one from the previous iterations\n",
    "    sample_output = pd.concat([sample_output, item_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bf1673-1ee7-44e3-bb47-10290ab305da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Item</th>\n",
       "      <th>Similar Items</th>\n",
       "      <th>Score Similitud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tênis Olympikus Esporte Valente - Masculino Kids</td>\n",
       "      <td>Tênis Infantil Olympikus Valente Kids Masculino</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tênis Olympikus Esporte Valente - Masculino Kids</td>\n",
       "      <td>Tênis Esporte Masculino</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tênis Olympikus Esporte Valente - Masculino Kids</td>\n",
       "      <td>Tênis Kids Masculino Esportivo Olympikus V-zero</td>\n",
       "      <td>0.617213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tênis Olympikus Esporte Valente - Masculino Kids</td>\n",
       "      <td>Tenis Infantil Masculino Olympikus Spider Kids...</td>\n",
       "      <td>0.617213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tênis Olympikus Esporte Valente - Masculino Kids</td>\n",
       "      <td>Tênis Esporte Olympikus Veloz</td>\n",
       "      <td>0.612372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...</td>\n",
       "      <td>Bicicleta Barra Forte Aro 24</td>\n",
       "      <td>0.516398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...</td>\n",
       "      <td>Bicicleta Samy Cross Aro 20 C/ Aros Aero</td>\n",
       "      <td>0.510310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...</td>\n",
       "      <td>Bicicleta Sport Aro 29 C/ Susp. 21v C/shim Tam...</td>\n",
       "      <td>0.503953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...</td>\n",
       "      <td>Bicicleta Sport Aro 29 C/ Susp. 21 V C/shim Ta...</td>\n",
       "      <td>0.492366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...</td>\n",
       "      <td>Bicicleta 29 Elleven Rocker 24v Hidráulico C/ ...</td>\n",
       "      <td>0.456435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Labrador 2</td>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Lunna Labrad...</td>\n",
       "      <td>0.935414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Labrador 2</td>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Pug 6</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Labrador 2</td>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Violões 3</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Labrador 2</td>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Fusca 4</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Labrador 2</td>\n",
       "      <td>Tênis Usthemp Slip-on Temático - Unigothic 2</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tênis Casual Feminino Moleca Tecido Tie Dye</td>\n",
       "      <td>Tênis Tie Dye Feminino Casual</td>\n",
       "      <td>0.845154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tênis Casual Feminino Moleca Tecido Tie Dye</td>\n",
       "      <td>Tênis Casual Feminino Moleca Tie Dye 5696203 C...</td>\n",
       "      <td>0.801784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tênis Casual Feminino Moleca Tecido Tie Dye</td>\n",
       "      <td>Tênis Casual Feminino Tie-dye Comfy</td>\n",
       "      <td>0.771517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tênis Casual Feminino Moleca Tecido Tie Dye</td>\n",
       "      <td>Tenis Feminino Casual Plataforma Tie Dye</td>\n",
       "      <td>0.771517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tênis Casual Feminino Moleca Tecido Tie Dye</td>\n",
       "      <td>Tênis Feminino Colorido Tecido Tie Dye</td>\n",
       "      <td>0.771517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tênis Star Baby Sapatinho Conforto + Brinde</td>\n",
       "      <td>Tênis Conforto</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tênis Star Baby Sapatinho Conforto + Brinde</td>\n",
       "      <td>Tenis Feminino Star Conforto Tie Dye</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tênis Star Baby Sapatinho Conforto + Brinde</td>\n",
       "      <td>Tênis Infantil Ortopé Sport Baby Sapatinho Beb...</td>\n",
       "      <td>0.492366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tênis Star Baby Sapatinho Conforto + Brinde</td>\n",
       "      <td>Tenis Star Feet</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tênis Star Baby Sapatinho Conforto + Brinde</td>\n",
       "      <td>Tênis Masculino + Brinde</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Target Item  \\\n",
       "0   Tênis Olympikus Esporte Valente - Masculino Kids   \n",
       "1   Tênis Olympikus Esporte Valente - Masculino Kids   \n",
       "2   Tênis Olympikus Esporte Valente - Masculino Kids   \n",
       "3   Tênis Olympikus Esporte Valente - Masculino Kids   \n",
       "4   Tênis Olympikus Esporte Valente - Masculino Kids   \n",
       "0  Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...   \n",
       "1  Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...   \n",
       "2  Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...   \n",
       "3  Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...   \n",
       "4  Bicicleta Barra Forte Samy C/ 6 Marchas Cubo C...   \n",
       "0       Tênis Usthemp Slip-on Temático - Labrador 2   \n",
       "1       Tênis Usthemp Slip-on Temático - Labrador 2   \n",
       "2       Tênis Usthemp Slip-on Temático - Labrador 2   \n",
       "3       Tênis Usthemp Slip-on Temático - Labrador 2   \n",
       "4       Tênis Usthemp Slip-on Temático - Labrador 2   \n",
       "0        Tênis Casual Feminino Moleca Tecido Tie Dye   \n",
       "1        Tênis Casual Feminino Moleca Tecido Tie Dye   \n",
       "2        Tênis Casual Feminino Moleca Tecido Tie Dye   \n",
       "3        Tênis Casual Feminino Moleca Tecido Tie Dye   \n",
       "4        Tênis Casual Feminino Moleca Tecido Tie Dye   \n",
       "0        Tênis Star Baby Sapatinho Conforto + Brinde   \n",
       "1        Tênis Star Baby Sapatinho Conforto + Brinde   \n",
       "2        Tênis Star Baby Sapatinho Conforto + Brinde   \n",
       "3        Tênis Star Baby Sapatinho Conforto + Brinde   \n",
       "4        Tênis Star Baby Sapatinho Conforto + Brinde   \n",
       "\n",
       "                                       Similar Items  Score Similitud  \n",
       "0   Tênis Infantil Olympikus Valente Kids Masculino          0.833333  \n",
       "1                            Tênis Esporte Masculino         0.707107  \n",
       "2  Tênis Kids Masculino Esportivo Olympikus V-zero           0.617213  \n",
       "3  Tenis Infantil Masculino Olympikus Spider Kids...         0.617213  \n",
       "4                      Tênis Esporte Olympikus Veloz         0.612372  \n",
       "0                      Bicicleta Barra Forte Aro 24          0.516398  \n",
       "1           Bicicleta Samy Cross Aro 20 C/ Aros Aero         0.510310  \n",
       "2  Bicicleta Sport Aro 29 C/ Susp. 21v C/shim Tam...         0.503953  \n",
       "3  Bicicleta Sport Aro 29 C/ Susp. 21 V C/shim Ta...         0.492366  \n",
       "4  Bicicleta 29 Elleven Rocker 24v Hidráulico C/ ...         0.456435  \n",
       "0  Tênis Usthemp Slip-on Temático - Lunna Labrad...         0.935414  \n",
       "1            Tênis Usthemp Slip-on Temático - Pug 6         0.857143  \n",
       "2        Tênis Usthemp Slip-on Temático - Violões 3         0.857143  \n",
       "3          Tênis Usthemp Slip-on Temático - Fusca 4         0.857143  \n",
       "4      Tênis Usthemp Slip-on Temático - Unigothic 2         0.857143  \n",
       "0                     Tênis Tie Dye Feminino Casual          0.845154  \n",
       "1  Tênis Casual Feminino Moleca Tie Dye 5696203 C...         0.801784  \n",
       "2                Tênis Casual Feminino Tie-dye Comfy         0.771517  \n",
       "3          Tenis Feminino Casual Plataforma Tie Dye          0.771517  \n",
       "4             Tênis Feminino Colorido Tecido Tie Dye         0.771517  \n",
       "0                                    Tênis Conforto          0.577350  \n",
       "1              Tenis Feminino Star Conforto Tie Dye          0.500000  \n",
       "2  Tênis Infantil Ortopé Sport Baby Sapatinho Beb...         0.492366  \n",
       "3                                   Tenis Star Feet          0.471405  \n",
       "4                           Tênis Masculino + Brinde         0.471405  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dropping any itens have the same item description - otherwise we woudn't need this algorithm\n",
    "sample_output = sample_output.loc[sample_output['Target Item'] != sample_output['Similar Items']]\n",
    "\n",
    "## taking a look at the results\n",
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f9dba3-cfa4-4e04-a5d6-66d0e593e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the results locally\n",
    "if not os.path.exists(path = '../outputs/'):\n",
    "    os.mkdir(path = '../outputs/')\n",
    "sample_output.to_csv(path_or_buf = '../outputs/sample_output_challenge_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a6503-fdf8-4b83-9e20-e4db32428f27",
   "metadata": {},
   "source": [
    "## Where to go from here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e69d13-7ad3-4250-8d97-b3104adfbf89",
   "metadata": {},
   "source": [
    "My idea is that this very simple algorithm could be used to measure at least to things: (1) classification accuracy and (2) computation time. I would then take their values as baselines to this problem, and try to improve on them attempting any of the following (and/or their combinations):\n",
    "\n",
    "+ Pre-calculate the vector representation of the items in the training data and use them in the `score_similitud` function above instead of having to do it all from scratch. The main advantage of this feature would be to speed up the inference time for the function;  \n",
    "+ Pre-calculate not only the vector representations from the training data but also their L2 norm for use in the denominator for the cosine similiarity. This feature should also speed up the inference time;\n",
    "+ Replace the `for` loop for a vectorized version of the operation - _e.g._, use a dot product between the target item vector representation in matrix format and the vector represetation of the other items. This should speed up the inference time considerably.\n",
    "+ Use pre-trained word vectors instead of the simple average of the one hot encoded representation of each word;  \n",
    "+ Use a locally sensitive hashing or some technique alike to partition the itens into smallers groups, and then try to conduct item similarity search within each of these hash buckets;   \n",
    "+ Manually annotate a few instances from the training dataset and use it to train a supervised algorithm (_e.g._, a siamese network);  \n",
    "+ Attempt to automate the data annotation process to train a supervised algorithm by:  \n",
    "    * Randomly selecting an item and:\n",
    "        * Creating a copy of it that would be lacking from 0 to k of its words. This copy and the original item would be taken as instances representing similar itens;  \n",
    "        * Randomly picking another item and dropping from 0 to k of its words. This copy and the original item would be taken as instances representing dissimilar itens.\n",
    "    * Then, I would create some new training data from the item pairs created on the above step; and,\n",
    "    * I would them train a supervised algorithm on top of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15375356-7437-49d0-ab4e-ad99c6ab2788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
